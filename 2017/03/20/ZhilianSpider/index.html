<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>我把我知道的Scrapy都告诉你 - 何小贱</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="最近在找工作，发现每天智联上招聘iOS的并不多，于是我就很好奇，是我投递的位置不对嘛？于是本着好奇心，于是决定爬取一下智联上相关的信息。同时也是第一次使用Scrapy，来这里也算是记录一下学习经过。">
<meta property="og:type" content="article">
<meta property="og:title" content="我把我知道的Scrapy都告诉你">
<meta property="og:url" content="http://yoursite.com/2017/03/20/ZhilianSpider/index.html">
<meta property="og:site_name" content="何小贱">
<meta property="og:description" content="最近在找工作，发现每天智联上招聘iOS的并不多，于是我就很好奇，是我投递的位置不对嘛？于是本着好奇心，于是决定爬取一下智联上相关的信息。同时也是第一次使用Scrapy，来这里也算是记录一下学习经过。">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-70926c5c27e4e11d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-49b777b766585cbd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-887d0c0433791415.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-203de4178aa933ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-17fe0b3622400f27.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-adf85ba013ae3522.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-6f9bd98e8cb346b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-dbc9e19b21f31fd4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-7dcd8f8e9d083cb0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-c4ae5a4966f61166.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-7632e540c753b0c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-6cb51a6baa530118.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-618af501e7cb3d98.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-5e56a74da8f869e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-0e4409d2d843e0dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-d76746c5d0e92e29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-7394e07b1686e36a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-571ccb8124b18b3f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-1e0b6c11a504e355.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-5461503ebdff9962.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-64c7b19da59e7d1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-be42cf361b5b6027.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-53ef2e6ca8fb84fd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-63c066290ab320bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-d3ab08afb569abc8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-d6b3407b683856e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-4541a708df8cc281.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-55389ba32740c966.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-d86bbf540c359594.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-e867b55740851d0b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-75228f0ed4febea5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-8508c09c425e461f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-78df69eefcaee243.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-3f2097256cb34912.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-5ca58c68c492b769.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-b2f2f04b49bfca7f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-fc87cf46121c1027.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-3b12da858ca99a96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-a7468b6d04877453.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-ebb3838ef5759399.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-2ed69f7c65c05d47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-d1271616e055ba2f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-a9301d729dd6f827.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-f71ef9ba1471d3a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-62be3bab33013e66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-25ca13b42aa01241.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-76dc4f4a9513dc96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2017-03-20T09:26:08.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="我把我知道的Scrapy都告诉你">
<meta name="twitter:description" content="最近在找工作，发现每天智联上招聘iOS的并不多，于是我就很好奇，是我投递的位置不对嘛？于是本着好奇心，于是决定爬取一下智联上相关的信息。同时也是第一次使用Scrapy，来这里也算是记录一下学习经过。">
<meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/1503554-70926c5c27e4e11d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
  
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>
    <section id="main" class="outer"><article id="post-ZhilianSpider" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      我把我知道的Scrapy都告诉你
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2017/03/20/ZhilianSpider/" class="article-date">
  <time datetime="2017-03-20T09:25:38.000Z" itemprop="datePublished">2017-03-20</time>
</a>
      
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近在找工作，发现每天智联上招聘iOS的并不多，于是我就很好奇，是我投递的位置不对嘛？于是本着好奇心，于是决定爬取一下智联上相关的信息。同时也是第一次使用Scrapy，来这里也算是记录一下学习经过。<br><a id="more"></a></p>
<p>言归正传，开始我们的表演。</p>
<blockquote>
<p>这里关与Scrapy、numpy、matplotlib的安装就不介绍了，可以去网上查一下。</p>
</blockquote>
<p>#####第一步，我们来创建一个Scrapy项目<br>我们来看一下Scrapy文档<br><a href="https://doc.scrapy.org/en/1.3/intro/tutorial.html#how-to-run-our-spider" target="_blank" rel="external">https://doc.scrapy.org/en/1.3/intro/tutorial.html#how-to-run-our-spider</a></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-70926c5c27e4e11d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>OK，搞起。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">bogon:Scrapy mac$ cd /.../文件夹/ #项目存放的文件夹</div><div class="line">#项目名称为zhilian_job</div><div class="line">bogon:Scrapy mac$ scrapy startproject zhilian_job</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-49b777b766585cbd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>如果你看到这些那么就已经成功了，是不是很简单。</p>
<p>#####第二步，我们创建一个Spider<br>我们进入到zhilian_job文件夹下的spider文件夹下。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#进入到spider文件夹</div><div class="line">bogon:Scrapy mac$ cd zhilian_job/</div><div class="line">bogon:zhilian_job mac$ cd zhilian_job/</div><div class="line">bogon:zhilian_job mac$ cd spiders/</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-887d0c0433791415.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>我们会发现这里只有一个<strong>init</strong>.py文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#创建spider文件</div><div class="line">bogon:spiders mac$ vim zhilian_spider.py</div></pre></td></tr></table></figure></p>
<p>这里需要说一下，一个spider类的几个关键因素</p>
<blockquote>
<p>第一，需要继承自scrapy.Spider<br>第二，我们需要给他一个name  这个name当我们执行scrapy crawl的时候会用到<br>第三，我们需要给他一个start_urls，这是一个列表，也就意味着我们可以有多个start_urls<br>第四，需要有一个参数是response的parse方法，其实这个方法名字可改变，这个后面在说。</p>
</blockquote>
<p>大概是这个样子。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line"></div><div class="line">class ZhilianSpider(scrapy.Spider):</div><div class="line">#排从需要的名字</div><div class="line"> name = &apos;zhilian_spider&apos;</div><div class="line">#开始的url</div><div class="line">    start_urls = [</div><div class="line">        &apos;&apos;</div><div class="line">    ]</div><div class="line"> </div><div class="line">    def parse(self,response):</div><div class="line">        print (response)</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-203de4178aa933ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>#####第三步，爬取的网页<br>                           接下来我们来看一下我们要爬取的网站。<br>                           <img src="http://upload-images.jianshu.io/upload_images/1503554-17fe0b3622400f27.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>                           我们可以看到城市智联有个更多，因为要获取所有城市的iOS的招聘数量，所以我们需要先爬取这个网<a href="http://www.zhaopin.com/citymap.html" target="_blank" rel="external">http://www.zhaopin.com/citymap.html</a><br>                           我们将这个网址放到我们的start_urls中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">start_urls = [</div><div class="line">       &apos;http://www.zhaopin.com/citymap.html&apos;</div><div class="line">   ]</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-adf85ba013ae3522.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>这样我们的第一个Scrapy就算是写完了，这就完了？这才是开始，不要着急，我们可以先跑起来看看，给自己一些继续的动力。<br>我们先回到第一个子目录也就是包含scrapy.cfg这个文件夹下。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">bogon:zhilian_job mac$ ls</div><div class="line">__init__.py	middlewares.py	settings.py</div><div class="line">items.py	pipelines.py	spiders</div><div class="line">bogon:zhilian_job mac$ cd ..</div><div class="line">bogon:zhilian_job mac$ ls</div><div class="line">scrapy.cfg	zhilian_job</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-6f9bd98e8cb346b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>然后我们执行 scrapy crawl zhilian_spider</p>
<blockquote>
<p>zhilian_spider ？ 好像在哪见过，是的，在我们ZhilianSpider类中的name 之前也有提过</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bogon:zhilian_job mac$ scrapy crawl zhilian_spider</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-dbc9e19b21f31fd4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>然后我们发现出来一大推东西，不要方，我们来找一下我们答应的response，额。。。 有点难找，我们来加点标记<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">bogon:zhilian_job mac$ vi zhilian_job/spiders/zhilian_spider.py</div><div class="line"></div><div class="line">def parse(self,response):</div><div class="line">	print (&apos;--------------&apos;)</div><div class="line">	print (response)</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-7dcd8f8e9d083cb0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>再来运行一下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bogon:zhilian_job mac$ scrapy crawl zhilian_spider</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-c4ae5a4966f61166.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>OK~看到这个就放心了。</p>
<p>#####第四步，解析数据<br>根据文档，我们可以使用xpath 或者 css来解析，其实貌似还可以使用BeautifulSoup，文档中极力推荐使用xpath，于是我们来用一下xpath。<br><a href="https://doc.scrapy.org/en/1.3/topics/selectors.html#using-selectors" target="_blank" rel="external">https://doc.scrapy.org/en/1.3/topics/selectors.html#using-selectors</a><br>可以来这里学习一下。<br><img src="http://upload-images.jianshu.io/upload_images/1503554-7632e540c753b0c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>然后在看一下结构。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-6cb51a6baa530118.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br><img src="http://upload-images.jianshu.io/upload_images/1503554-618af501e7cb3d98.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>可以看到这是一个list嵌套一个list<br>OK，让我们来解析一下数据。<br>回到我们的spider中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bogon:zhilian_job mac$ vi zhilian_job/spiders/zhilian_spider.py</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">def parse(self,response):</div><div class="line">	for citys in response.xpath(&apos;//div[@id=&quot;letter_choose&quot;]/dl/dd&apos;):</div><div class="line">		for city in citys.xpath(&apos;.//a&apos;):</div><div class="line">			#因为有可能有加粗的情况所以需要加个判断</div><div class="line">				if city.xpath(&apos;.//strong&apos;).extract_first() is not None:</div><div class="line">					city = city.xpath(&apos;.//strong&apos;)</div><div class="line">				ct = city.xpath(&apos;.//text()&apos;).extract_first()</div><div class="line">				print (ct)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-5e56a74da8f869e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<blockquote>
<p>如果你觉着xpath不好写，这里有方法，是我查资料是不经意间发现的。<br>比如想知道id = letter_choose这个标签，又不想写他的xpath，我们可以使用右键然后copy Xpath来获取<br><img src="http://upload-images.jianshu.io/upload_images/1503554-0e4409d2d843e0dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
</blockquote>
<p>OK，解析弄完之后我们看一下我们有没有获取到结果<br>回到包含scrapy.cfg的文件夹下执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bogon:zhilian_job mac$ scrapy crawl zhilian_spider</div></pre></td></tr></table></figure></p>
<p>查看结果，发现确实打印了，我们需要的城市。<br><img src="http://upload-images.jianshu.io/upload_images/1503554-d76746c5d0e92e29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>我们离成功更近一步了。</p>
<p>#####接下来第五步，我们获取城市还没完，我们还要查看招聘信息。</p>
<blockquote>
<p>这之前需要说一个问题，就是之前说的parse这个方法的问题。其实我们看文档会发现，我们本可以不使用parse这个方法。</p>
</blockquote>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-7394e07b1686e36a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>这是文档中的一段，我们不难发现这里调用了<br>scrapy.Request(url = url ,callback=self.parse)<br>第一个参数就是我们想要请求的url。<br>第二个参数是我们需要回调的方法。</p>
<p>通过这个方法我们可以做什么呢？我们来看一下搜索济南iOS的url是什么。<br><a href="http://sou.zhaopin.com/jobs/searchresult.ashx?jl=%E6%B5%8E%E5%8D%97&amp;kw=ios&amp;sm=0&amp;p=1" target="_blank" rel="external">http://sou.zhaopin.com/jobs/searchresult.ashx?jl=%E6%B5%8E%E5%8D%97&amp;kw=ios&amp;sm=0&amp;p=1</a><br>这里需要四个参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">jl 城市名称</div><div class="line">kw 搜索关键字</div><div class="line">sm = 0</div><div class="line">p page第几页，如果搜索结果又多页可修改此参数</div></pre></td></tr></table></figure></p>
<p>所以我们只需要修改城市就可以获取到每个城市的iOS的招聘情况。<br>让我们来试一下。回到spider中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vi zhilian_job/spiders/zhilian_spider.py</div></pre></td></tr></table></figure></p>
<p>######先修改一下parse方法，方法中这里我们也用到了scrapy.Request方法，然后我们在添加一个爬取每个城市的parse方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">def parse(self,response):</div><div class="line">	for citys in response.xpath(&apos;//div[@id=&quot;letter_choose&quot;]/dl/dd&apos;):</div><div class="line">		for city in citys.xpath(&apos;.//a&apos;):</div><div class="line">		    if city.xpath(&apos;.//strong&apos;).extract_first() is not None:</div><div class="line">				city = city.xpath(&apos;.//strong&apos;)</div><div class="line">		    ct = city.xpath(&apos;.//text()&apos;).extract_first()</div><div class="line">		    url_city = &apos;http://sou.zhaopin.com/jobs/searchresult.ashx?jl=%s&amp;    kw=ios&amp;sm=0&amp;p=1&apos; % ct                     </div><div class="line">		    yield scrapy.Request(url=url_city, callback=self.parse_jobs)</div><div class="line"></div><div class="line">def parse_jobs(self,response):</div><div class="line">	pass</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-571ccb8124b18b3f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>跑一下结果.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bogon:zhilian_job mac$ scrapy crawl zhilian_spider</div></pre></td></tr></table></figure></p>
<p>这里我终止了爬取，因为我们可以看到parse_job方法已经执行了，成功了一半了！<br><img src="http://upload-images.jianshu.io/upload_images/1503554-1e0b6c11a504e355.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>#####第六步，我们来解析一下搜索结果的网页</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-5461503ebdff9962.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>这里面就是我们需要的东西，放在<tr>中，放在<table>里，很明显是一个list。<br>继续更新一下parse_jobs方法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#导入头文件</div><div class="line">from zhilian_job.items import ZhilianJobItem</div><div class="line">#修改parse方法</div><div class="line">def parse_jobs(self,response):</div><div class="line">	job_position = response.xpath(&apos;//*[@id=&quot;JobLocation&quot;]/@value&apos;).extract_first()       </div><div class="line">	for job in response.xpath(&apos;//table[@class=&quot;newlist&quot;]/tr&apos;):</div><div class="line">		kword = job.xpath(&apos;.//td[@class=&quot;zwmc&quot;]/div/a/b/text()&apos;).extract_first()</div><div class="line">		if kword is not None:</div><div class="line">			item = ZhilianItem()</div><div class="line">			item[&quot;job_company&quot;] = job.xpath(&apos;.//td[@class=&quot;gsmc&quot;]/a/text()&apos;).extract_first()</div><div class="line">			item[&quot;job_price&quot;] = job.xpath(&apos;.//td[@class=&quot;zwyx&quot;]/text()&apos;)    .extract_first()</div><div class="line">			item[&quot;job_date&quot;] = job.xpath(&apos;.//td[@class=&quot;gxsj&quot;]/span/text    ()&apos;).extract_first()</div><div class="line">			item[&quot;job_name&quot;] = job.xpath(&apos;.//td[@class=&quot;zwmc&quot;]/div/a/tex    t()&apos;).extract_first()</div><div class="line">			item[&quot;job_position&quot;] = job_position</div><div class="line">			item[&quot;job_is&quot;] = 1</div><div class="line">			yield item</div></pre></td></tr></table></figure></table></tr></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-64c7b19da59e7d1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>关于item的使用还请各位看一下文档，不是很难。<br><a href="https://doc.scrapy.org/en/1.3/topics/items.html" target="_blank" rel="external">https://doc.scrapy.org/en/1.3/topics/items.html</a><br>这里贴一下我的items.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">from scrapy.item import Item,Field</div><div class="line"></div><div class="line">class ZhilianJobItem(Item):</div><div class="line">	job_name = Field()</div><div class="line">	job_company = Field()</div><div class="line">	job_price = Field()</div><div class="line">	job_position = Field()</div><div class="line">	job_date = Field()</div><div class="line">	job_is = Field()</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-be42cf361b5b6027.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>运行一下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl zhilian_spider</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-53ef2e6ca8fb84fd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>这里我中断了，因为已经看到了想要的数据。</p>
<p>#####第七步 处理下一页问题<br>如果仔细分析，不难发现，我们只获取到了每个城市的第一页的数据，这很尴尬，如果有下一页怎么办，我们来处理一下。</p>
<p>先看一下网页，找到下一页<br><img src="http://upload-images.jianshu.io/upload_images/1503554-63c066290ab320bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>让我们回调parse_jobs方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">#后去下一页的网址</div><div class="line">job_next = response.xpath(&apos;//a[@class=&quot;next-page&quot;]/@href&apos;).extract_first()</div><div class="line">#打印一下结果，因为之前并不知道job_next是什么值，所以打印一下看一下结果，发现如果没有就是None</div><div class="line">print (&quot;********&quot;,job_next)</div><div class="line">#如果有下一页就爬取下一页</div><div class="line">if job_next is not None:</div><div class="line">	#发送网络请求，并callback自己</div><div class="line">	yield scrapy.Request(url=job_next, callback=self.parse_jobs)</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-d3ab08afb569abc8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<blockquote>
<p>关键在yield那句  callback我们回调的还是parse_jobs()方法</p>
</blockquote>
<p>Ok，这样我们可以看一下结果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl zhilian_spider</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-d6b3407b683856e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>没有问题。</p>
<p>那我们如何保存成文件呢。<br>只需要一句话<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bogon:zhilian_job mac$ scrapy crawl zhilian_spider -o items.json</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-4541a708df8cc281.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>OK,到这第一部分就完成了。</p>
<blockquote>
<p>补充，当你点开json文件，你会发现怎么全是unicode编码，说句实话 ，我也不知道怎么处理，使用item就是看到可以通过item解决，自己试了一下，并没有成功，但是这不影响后面的操作，如果有知道处理方式的可以共享一下。</p>
</blockquote>
<p>下面我们会处理一下数据然后显示出来，敬请期待。</p>
<p>———————–有待更新————————-</p>
<p>OK，让我们继续下面的表演。</p>
<p>经过了大概半个小时，我终于看到了这个界面。恭喜你，你成功了，恭喜你，你被我坑了，哈哈，为什么这么说呢，下面给你解释。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-55389ba32740c966.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>可以看到item_scraped_count是326153，也就是说你有一个32W+的一个数据文件了。</p>
<p>我们的目的是将其显示成图，哇，要显示这么多嘛？不是的，我们还需要处理，于是我们就需要使用pandas，做图必然要使用matplotlib。</p>
<p>这里我们只需要使用ipython就Ok了.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">In [1]: import pandas as pd</div><div class="line">#注意这里我用用到的是pyplot 这里一定要引入matplotlib,后面会解释</div><div class="line">In [2]: import matplotlib.pyplot as plt</div><div class="line"></div><div class="line">In [3]: import numpy as np</div><div class="line"></div><div class="line">In [4]: items = pd.read_json(&apos;items.json&apos;)</div><div class="line">#回车后你会发现有好多数据</div><div class="line">In [5]: items</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-d86bbf540c359594.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>当时当我看到全国两个字的时候，我方了，为什么会有全国呢，我们不需要啊，而且你看到后面你会发现，我们估计花儿一半多时间才将全国爬取完成。也对不住各位坑你们了，让你们花了这么长时间，不过我感觉看着屏幕一直刷的感觉，像是在看骇客帝国，哈哈。其次，这样才有意思不是嘛？各种突发情况才更有趣。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-e867b55740851d0b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>下面再说一下我们的需求</p>
<blockquote>
<p>我们需要获取到每个城市的平均简历工资，我们需要获取到每个城市的招聘数量。</p>
</blockquote>
<p>#####下面开始我们的数据处理的第一步，去重<br>接下来我们要做的就是去重，依然使用pandas,我是在这篇文章中看到的。<a href="https://zhuanlan.zhihu.com/p/25473751？group_id=819874677425053696" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/25473751？group_id=819874677425053696</a></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-75228f0ed4febea5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>Ok来执行以下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">In [7]: items.drop_duplicates([&apos;job_name&apos;,&apos;job_position&apos;],keep=&apos;last&apos;,inplace=True)</div><div class="line"></div><div class="line">In [8]: items</div></pre></td></tr></table></figure></p>
<p>神奇的事情发生了，32W + 变成了4k+，看到这里，我貌似明白了什么。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-8508c09c425e461f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>我们不难发现在处理工资这里的时候会有一些问题，有的是面议，其次就是xxxx - xxxx这样肯定算不出平均工资吧，我们还要处理一下。</p>
<p>#####第二步，数据再次处理</p>
<p>当我纠结怎么办的时候，我突然发现文档里有一个叫做item pipeline的东西，貌似可以在里面处理再次处理数据。</p>
<p>我们在我们的Scrapy项目中找一下，发现在items.py同级目录下，有一个pipelines.py,既然加s说明可以有多个pipeline。</p>
<p>进去看一下，建议不要关闭ipython,重新打开一个终端。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">#导入re是因为发现工资是以字符串返回的，同事我们需要匹配格式为xxx-xxx的工资</div><div class="line">import re</div><div class="line">from scrapy.exceptions import DropItem</div><div class="line">	 </div><div class="line">class ZhilianJobPipeline(object):</div><div class="line">	def process_item(self, item, spider):</div><div class="line">		price = item[&quot;job_price&quot;]</div><div class="line">		ma = re.match(r&apos;\d+-\d+&apos;,price)</div><div class="line">		if ma is not None:</div><div class="line">			#通过字符串的split方法将字符串分割获取得到最低与最高工资</div><div class="line">			prices = price.split(&apos;-&apos;) </div><div class="line">			#这里需要转换一下，转换成int类型，然后算一下平均值</div><div class="line">			price = (int(prices[0]) + int(prices[1])) / 2</div><div class="line">			item[&apos;job_price&apos;] = price</div><div class="line">			#这样我们就获取到了每个招聘信息的平均价格。</div><div class="line">			return item</div><div class="line">		else:</div><div class="line">			raise DropItem(&quot;Missing price in %s&quot; % item)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-78df69eefcaee243.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>这里还需要配置一个地方，我们需要设置setting.py里的ITEM_PIPELINE这个地方。文档最下面可以看到。因为我们这只有一个，所以我们只需要打开注释就好。<br><img src="http://upload-images.jianshu.io/upload_images/1503554-3f2097256cb34912.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-5ca58c68c492b769.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>这样就ok，保存一下，我们再来执行一下crawl zhilian_spider吧，不是吧，又要30分钟，是的，我当时就这样做的。想想也很无奈，不过第一次也是难免的。</p>
<p>开始前最好将之前的items.json删除。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Sheldon:zhilian_job mac$ rm -rf items.json </div><div class="line">Sheldon:zhilian_job mac$ scrapy crawl zhilian_spider -o items.json</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-b2f2f04b49bfca7f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>没问题，这正是我们想要的。</p>
<p>#####第三步，数据整理<br>回到第一步使用ipython对items进行去重，这里就不重复了。</p>
<p>因为我们需要的是每个地区的所有招聘数量与平均工资，所以我们还需要对每个地区进行处理，还好在<a href="https://zhuanlan.zhihu.com/p/25473751?group_id=819874677425053696" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/25473751?group_id=819874677425053696</a><br>这篇文章中我发现了一个叫做groupby的方法，貌似可以拿来用一下。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-fc87cf46121c1027.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>我们需要对position列进行分组，然后获取price的平均价格<br>这里是获取平均值的方法，<a href="http://www.aichengxu.com/python/16058.htm" target="_blank" rel="external">http://www.aichengxu.com/python/16058.htm</a></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-3b12da858ca99a96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>同时我们还需要显示个地区的所有招聘数量，这个可以使用count()方法。<br><img src="http://upload-images.jianshu.io/upload_images/1503554-a7468b6d04877453.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">In [13]: job_price = items.groupby(&apos;job_position&apos;)[&apos;job_price&apos;].mean()</div><div class="line"></div><div class="line">In [14]: job_num = items.groupby(&apos;job_position&apos;)[&apos;job_price&apos;].count()</div><div class="line"></div><div class="line">In [15]: job_num</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-ebb3838ef5759399.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>数据我们算是初步处理完成，下面就是显示问题了，这里遇到一个坑，花了半天时间，真的是半天。</p>
<p>去看文档，上面写绘制图需要用到plot方法<br><a href="http://pandas.pydata.org/pandas-docs/stable/visualization.html" target="_blank" rel="external">http://pandas.pydata.org/pandas-docs/stable/visualization.html</a><br>然后我就试了一下</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-2ed69f7c65c05d47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<blockquote>
<p>然后呢？我没看到图啊 ,文档输入完plot()就显示图了，为什么我没有？</p>
</blockquote>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-d1271616e055ba2f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<blockquote>
<p>然后找了半天资料，发现需要执行一下plt.show()这也是为什么一开始带入头文件时说一定要有plt。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">In [17]: plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-a9301d729dd6f827.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>看到这张图的时候差点泪崩，文档有时候也骗人。不过仔细一看，就真的泪崩了，这是些啥？搞了这么就，就弄出来这个？其实细想其实是地区太多导致，显示不开。<br>于是我们可以改一下，只获取每个省的数量，这样的好处就是数量不多，同时，可以显示的开。于是回到spider中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">def parse(self,response):</div><div class="line">		  </div><div class="line">	citys = [&apos;北京&apos;,&apos;天津&apos;,&apos;上海&apos;,&apos;重庆&apos;,&apos;河北&apos;,&apos;山西&apos;,&apos;辽宁&apos;,&apos;吉林&apos;,&apos;黑</div><div class="line">		      龙江&apos;,&apos;江苏&apos;,&apos;浙江&apos;,&apos;安徽&apos;,&apos;福建&apos;,&apos;江西&apos;,&apos;山东&apos;,&apos;河南&apos;,&apos;湖北&apos;,&apos;湖南&apos;,&apos;广东&apos;,    &apos;海南&apos;,&apos;四川&apos;,&apos;贵州&apos;,&apos;云南&apos;,&apos;陕西&apos;,&apos;甘肃&apos;,&apos;青海&apos;,&apos;台湾&apos;,&apos;内蒙古&apos;,&apos;广西&apos;,&apos;西&gt;    藏&apos;,&apos;宁夏&apos;,&apos;新疆&apos;,&apos;香港&apos;,&apos;澳门&apos;]</div><div class="line">	for ct in citys:</div><div class="line">		#for citys in response.xpath(&apos;//div[@id=&quot;letter_choose&quot;]/dl/dd&apos;):   </div><div class="line">		   #for city in citys.xpath(&apos;.//a&apos;):</div><div class="line">				#if city.xpath(&apos;.//strong&apos;).extract_first() is not None:</div><div class="line">					#city = city.xpath(&apos;.//strong&apos;)</div><div class="line">		        #ct = city.xpath(&apos;.//text()&apos;).extract_first()</div><div class="line">		url_city = &apos;http://sou.zhaopin.com/jobs/searchresult.ashx?jl=%s&amp;    kw=ios&amp;sm=0&amp;p=1&apos; % ct</div><div class="line">        yield scrapy.Request(url=url_city, callback=self.parse_jobs)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-f71ef9ba1471d3a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>抓取结果:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-62be3bab33013e66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>不难发现，确实少了很多。<br>然后去重，分租，就不重复了<br>看一下最终结果:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-25ca13b42aa01241.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>这个是招聘平均工资，澳门香港最高。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-76dc4f4a9513dc96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>看到整个山东加起来都不到150，我也就放心了。</p>
<p>————————–  补充   ———————————</p>
<p>scrapy 文档：<a href="https://doc.scrapy.org/en/1.3/intro/tutorial.html#how-to-run-our-spider" target="_blank" rel="external">https://doc.scrapy.org/en/1.3/intro/tutorial.html#how-to-run-our-spider</a><br>scrapy xpath文档：<a href="https://doc.scrapy.org/en/1.3/topics/selectors.html#topics-selectors" target="_blank" rel="external">https://doc.scrapy.org/en/1.3/topics/selectors.html#topics-selectors</a><br>scrapy  items文档：<a href="http://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/items.html" target="_blank" rel="external">http://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/items.html</a><br>scrapy与mongoDB的使用：<a href="https://realpython.com/blog/python/web-scraping-with-scrapy-and-mongodb/" target="_blank" rel="external">https://realpython.com/blog/python/web-scraping-with-scrapy-and-mongodb/</a><br>scrapy 去重：<a href="https://zhuanlan.zhihu.com/p/25473751?group_id=819874677425053696" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/25473751?group_id=819874677425053696</a><br>pandas使用：<a href="http://pandas.pydata.org/pandas-docs/stable/10min.html#min" target="_blank" rel="external">http://pandas.pydata.org/pandas-docs/stable/10min.html#min</a><br>pandas with matplotlib：<a href="http://pandas.pydata.org/pandas-docs/version/0.13.1/visualization.html" target="_blank" rel="external">http://pandas.pydata.org/pandas-docs/version/0.13.1/visualization.html</a><br>pandas 读取json错误处理：<a href="http://stackoverflow.com/questions/36837663/reading-json-file-as-pandas-dataframe-error" target="_blank" rel="external">http://stackoverflow.com/questions/36837663/reading-json-file-as-pandas-dataframe-error</a></p>
<p>####完！</p>

      
    </div>
    
    
      <footer class="article-footer">
        
      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2017/03/18/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Hello World&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>







</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 何小贱&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/ppoffice">PPOffice</a>
    </div>
  </div>
</footer>
    

<script src="/js/jquery.min.js"></script>



<script src="/js/script.js"></script>
  </div>
</body>
</html>