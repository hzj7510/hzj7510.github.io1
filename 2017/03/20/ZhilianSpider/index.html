<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>何小贱</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="最近在找工作，发现每天智联上招聘iOS的并不多，于是我就很好奇，是我投递的位置不对嘛？于是本着好奇心，于是决定爬取一下智联上相关的信息。同时也是第一次使用Scrapy，来这里也算是记录一下学习经过。
言归正传，开始我们的表演。

这里关与Scrapy、numpy、matplotlib的安装就不介绍了，可以去网上查一下。

#####第一步，我们来创建一个Scrapy项目我们来看一下Scrapy文档">
<meta property="og:type" content="article">
<meta property="og:title" content="何小贱">
<meta property="og:url" content="http://yoursite.com/2017/03/20/ZhilianSpider/index.html">
<meta property="og:site_name" content="何小贱">
<meta property="og:description" content="最近在找工作，发现每天智联上招聘iOS的并不多，于是我就很好奇，是我投递的位置不对嘛？于是本着好奇心，于是决定爬取一下智联上相关的信息。同时也是第一次使用Scrapy，来这里也算是记录一下学习经过。
言归正传，开始我们的表演。

这里关与Scrapy、numpy、matplotlib的安装就不介绍了，可以去网上查一下。

#####第一步，我们来创建一个Scrapy项目我们来看一下Scrapy文档">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-70926c5c27e4e11d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-49b777b766585cbd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-887d0c0433791415.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-17fe0b3622400f27.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/1503554-7632e540c753b0c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2017-03-20T08:52:52.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="何小贱">
<meta name="twitter:description" content="最近在找工作，发现每天智联上招聘iOS的并不多，于是我就很好奇，是我投递的位置不对嘛？于是本着好奇心，于是决定爬取一下智联上相关的信息。同时也是第一次使用Scrapy，来这里也算是记录一下学习经过。
言归正传，开始我们的表演。

这里关与Scrapy、numpy、matplotlib的安装就不介绍了，可以去网上查一下。

#####第一步，我们来创建一个Scrapy项目我们来看一下Scrapy文档">
<meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/1503554-70926c5c27e4e11d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
  
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
        </div>
      </nav>
    </div>
  </div>
</header>
    <section id="main" class="outer"><article id="post-ZhilianSpider" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
    <div class="article-meta">
      <a href="/2017/03/20/ZhilianSpider/" class="article-date">
  <time datetime="2017-03-20T08:52:52.000Z" itemprop="datePublished">2017-03-20</time>
</a>
      
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近在找工作，发现每天智联上招聘iOS的并不多，于是我就很好奇，是我投递的位置不对嘛？于是本着好奇心，于是决定爬取一下智联上相关的信息。同时也是第一次使用Scrapy，来这里也算是记录一下学习经过。</p>
<p>言归正传，开始我们的表演。</p>
<blockquote>
<p>这里关与Scrapy、numpy、matplotlib的安装就不介绍了，可以去网上查一下。</p>
</blockquote>
<p>#####第一步，我们来创建一个Scrapy项目<br>我们来看一下Scrapy文档<br><a href="https://doc.scrapy.org/en/1.3/intro/tutorial.html#how-to-run-our-spider" target="_blank" rel="external">https://doc.scrapy.org/en/1.3/intro/tutorial.html#how-to-run-our-spider</a></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-70926c5c27e4e11d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>OK，搞起。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">bogon:Scrapy mac$ cd /.../文件夹/ #项目存放的文件夹</div><div class="line">#项目名称为zhilian_job</div><div class="line">bogon:Scrapy mac$ scrapy startproject zhilian_job</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-49b777b766585cbd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>如果你看到这些那么就已经成功了，是不是很简单。</p>
<p>#####第二步，我们创建一个Spider<br>我们进入到zhilian_job文件夹下的spider文件夹下。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#进入到spider文件夹</div><div class="line">bogon:Scrapy mac$ cd zhilian_job/</div><div class="line">bogon:zhilian_job mac$ cd zhilian_job/</div><div class="line">bogon:zhilian_job mac$ cd spiders/</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/1503554-887d0c0433791415.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>我们会发现这里只有一个<strong>init</strong>.py文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#创建spider文件</div><div class="line">bogon:spiders mac$ vim zhilian_spider.py</div></pre></td></tr></table></figure></p>
<p>这里需要说一下，一个spider类的几个关键因素</p>
<blockquote>
<p>第一，需要继承自scrapy.Spider<br>第二，我们需要给他一个name  这个name当我们执行scrapy crawl的时候会用到<br>第三，我们需要给他一个start_urls，这是一个列表，也就意味着我们可以有多个start_urls<br>第四，需要有一个参数是response的parse方法，其实这个方法名字可改变，这个后面在说。</p>
</blockquote>
<p>大概是这个样子。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">1 import scrapy</div><div class="line">  2 </div><div class="line"> 3 class ZhilianSpider(scrapy.Spider):</div><div class="line">           #排从需要的名字</div><div class="line">    4     name = &apos;zhilian_spider&apos;</div><div class="line">	         #开始的url</div><div class="line">	  5     start_urls = [</div><div class="line">	    6         &apos;&apos;</div><div class="line">		  7     ]</div><div class="line">		    8 </div><div class="line">			  9     def parse(self,response):</div><div class="line">				   10         print (response)</div></pre></td></tr></table></figure></p>
<pre><code>![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-203de4178aa933ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
</code></pre><p>#####第三步，爬取的网页<br>                           接下来我们来看一下我们要爬取的网站。<br>                           <img src="http://upload-images.jianshu.io/upload_images/1503554-17fe0b3622400f27.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>                           我们可以看到城市智联有个更多，因为要获取所有城市的iOS的招聘数量，所以我们需要先爬取这个网<a href="http://www.zhaopin.com/citymap.html" target="_blank" rel="external">http://www.zhaopin.com/citymap.html</a><br>                           我们将这个网址放到我们的start_urls中。<br>                           <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"> 5     start_urls = [</div><div class="line">6         &apos;http://www.zhaopin.com/citymap.html&apos;</div><div class="line">  7     ]</div></pre></td></tr></table></figure></p>
<pre><code>![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-adf85ba013ae3522.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
这样我们的第一个Scrapy就算是写完了，这就完了？这才是开始，不要着急，我们可以先跑起来看看，给自己一些继续的动力。
我们先回到第一个子目录也就是包含scrapy.cfg这个文件夹下。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">bogon:zhilian_job mac$ ls</div><div class="line">__init__.py	middlewares.py	settings.py</div><div class="line">items.py	pipelines.py	spiders</div><div class="line">bogon:zhilian_job mac$ cd ..</div><div class="line">bogon:zhilian_job mac$ ls</div><div class="line">scrapy.cfg	zhilian_job</div></pre></td></tr></table></figure>

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-6f9bd98e8cb346b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
然后我们执行 scrapy crawl zhilian_spider
&gt;zhilian_spider ？ 好像在哪见过，是的，在我们ZhilianSpider类中的name 之前也有提过

<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bogon:zhilian_job mac$ scrapy crawl zhilian_spider</div></pre></td></tr></table></figure>

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-dbc9e19b21f31fd4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

然后我们发现出来一大推东西，不要方，我们来找一下我们答应的response，额。。。 有点难找，我们来加点标记
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">bogon:zhilian_job mac$ vi zhilian_job/spiders/zhilian_spider.py</div><div class="line"></div><div class="line">  9     def parse(self,response):</div><div class="line">    10         print (&apos;--------------&apos;)</div><div class="line">	 11         print (response)</div></pre></td></tr></table></figure>

          ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-7dcd8f8e9d083cb0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
          再来运行一下
          <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bogon:zhilian_job mac$ scrapy crawl zhilian_spider</div></pre></td></tr></table></figure>

          ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-c4ae5a4966f61166.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
          OK~看到这个就放心了。
</code></pre><p>#####第四步，解析数据<br>                                           根据文档，我们可以使用xpath 或者 css来解析，其实貌似还可以使用BeautifulSoup，文档中极力推荐使用xpath，于是我们来用一下xpath。<br>                                           <a href="https://doc.scrapy.org/en/1.3/topics/selectors.html#using-selectors" target="_blank" rel="external">https://doc.scrapy.org/en/1.3/topics/selectors.html#using-selectors</a><br>                                           可以来这里学习一下。<br>                                           <img src="http://upload-images.jianshu.io/upload_images/1503554-7632e540c753b0c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>                                           然后在看一下结构。</p>
<pre><code>                                       ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-6cb51a6baa530118.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
                                       ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-618af501e7cb3d98.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

                                       可以看到这是一个list嵌套一个list
                                       OK，让我们来解析一下数据。
                                       回到我们的spider中
                                       <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bogon:zhilian_job mac$ vi zhilian_job/spiders/zhilian_spider.py</div></pre></td></tr></table></figure>

                                       <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"> 9     def parse(self,response):</div><div class="line">10         for citys in response.xpath(&apos;//div[@id=&quot;letter_choose&quot;]/dl/dd&apos;):</div><div class="line"> 11             for city in citys.xpath(&apos;.//a&apos;):</div><div class="line">                    #因为有可能有加粗的情况所以需要加个判断</div><div class="line"> 12                 if city.xpath(&apos;.//strong&apos;).extract_first() is not None:</div><div class="line">  13                     city = city.xpath(&apos;.//strong&apos;)</div><div class="line">14                 ct = city.xpath(&apos;.//text()&apos;).extract_first()</div><div class="line"> 15                 print (ct)</div></pre></td></tr></table></figure>

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-5e56a74da8f869e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

&gt;如果你觉着xpath不好写，这里有方法，是我查资料是不经意间发现的。
比如想知道id = letter_choose这个标签，又不想写他的xpath，我们可以使用右键然后copy Xpath来获取
![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-0e4409d2d843e0dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

OK，解析弄完之后我们看一下我们有没有获取到结果
回到包含scrapy.cfg的文件夹下执行
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bogon:zhilian_job mac$ scrapy crawl zhilian_spider</div></pre></td></tr></table></figure>

查看结果，发现确实打印了，我们需要的城市。
![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-d76746c5d0e92e29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
我们离成功更近一步了。
</code></pre><p>#####接下来第五步，我们获取城市还没完，我们还要查看招聘信息。</p>
<pre><code>&gt;这之前需要说一个问题，就是之前说的parse这个方法的问题。其实我们看文档会发现，我们本可以不使用parse这个方法。

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-7394e07b1686e36a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这是文档中的一段，我们不难发现这里调用了
scrapy.Request(url = url ,callback=self.parse)
第一个参数就是我们想要请求的url。
第二个参数是我们需要回调的方法。

通过这个方法我们可以做什么呢？我们来看一下搜索济南iOS的url是什么。
http://sou.zhaopin.com/jobs/searchresult.ashx?jl=%E6%B5%8E%E5%8D%97&amp;kw=ios&amp;sm=0&amp;p=1
这里需要四个参数
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">jl 城市名称</div><div class="line">kw 搜索关键字</div><div class="line">sm = 0</div><div class="line">p page第几页，如果搜索结果又多页可修改此参数</div></pre></td></tr></table></figure>

所以我们只需要修改城市就可以获取到每个城市的iOS的招聘情况。
让我们来试一下。回到spider中。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vi zhilian_job/spiders/zhilian_spider.py</div></pre></td></tr></table></figure>
</code></pre><p>######先修改一下parse方法，方法中这里我们也用到了scrapy.Request方法，然后我们在添加一个爬取每个城市的parse方法<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"> 9     def parse(self,response):</div><div class="line">   10         for citys in response.xpath(&apos;//div[@id=&quot;letter_choose&quot;]/dl/dd&apos;):</div><div class="line">    11             for city in citys.xpath(&apos;.//a&apos;):</div><div class="line">    12                 if city.xpath(&apos;.//strong&apos;).extract_first() is not None:</div><div class="line">    13                     city = city.xpath(&apos;.//strong&apos;)</div><div class="line">    14                 ct = city.xpath(&apos;.//text()&apos;).extract_first()</div><div class="line">    15                 url_city = &apos;http://sou.zhaopin.com/jobs/searchresult.ashx?jl=%s&amp;    kw=ios&amp;sm=0&amp;p=1&apos; % ct</div><div class="line">                         </div><div class="line">    16                 yield scrapy.Request(url=url_city, callback=self.parse_jobs)</div><div class="line"></div><div class="line">18     def parse_jobs(self,response):</div><div class="line">  19         pass</div></pre></td></tr></table></figure></p>
<pre><code>![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-571ccb8124b18b3f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

跑一下结果.
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bogon:zhilian_job mac$ scrapy crawl zhilian_spider</div></pre></td></tr></table></figure>

这里我终止了爬取，因为我们可以看到parse_job方法已经执行了，成功了一半了！
![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-1e0b6c11a504e355.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
</code></pre><p>#####第六步，我们来解析一下搜索结果的网页</p>
<pre><code>![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-5461503ebdff9962.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这里面就是我们需要的东西，放在&lt;tr&gt;中，放在&lt;table&gt;里，很明显是一个list。
继续更新一下parse_jobs方法。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">  #导入头文件</div><div class="line">  2 from zhilian_job.items import ZhilianJobItem</div><div class="line"> #修改parse方法</div><div class="line">19     def parse_jobs(self,response):</div><div class="line">  20             job_position = response.xpath(&apos;//*[@id=&quot;JobLocation&quot;]/@value&apos;).extract_first() 20         </div><div class="line">             for job in response.xpath(&apos;//table[@class=&quot;newlist&quot;]/tr&apos;):</div><div class="line">   21             kword = job.xpath(&apos;.//td[@class=&quot;zwmc&quot;]/div/a/b/text()&apos;).extract_first()</div><div class="line">   22             if kword is not None:</div><div class="line">   23                 item = ZhilianItem()</div><div class="line">   24                 item[&quot;job_company&quot;] = job.xpath(&apos;.//td[@class=&quot;gsmc&quot;]/a/text()&apos;).extract_first()</div><div class="line">   25                 item[&quot;job_price&quot;] = job.xpath(&apos;.//td[@class=&quot;zwyx&quot;]/text()&apos;)    .extract_first()</div><div class="line">   26                 item[&quot;job_date&quot;] = job.xpath(&apos;.//td[@class=&quot;gxsj&quot;]/span/text    ()&apos;).extract_first()</div><div class="line">   27                 item[&quot;job_name&quot;] = job.xpath(&apos;.//td[@class=&quot;zwmc&quot;]/div/a/tex    t()&apos;).extract_first()</div><div class="line">   28                 item[&quot;job_position&quot;] = job_position</div><div class="line">   29                 item[&quot;job_is&quot;] = 1</div><div class="line">   30                 yield item</div></pre></td></tr></table></figure>

        ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-64c7b19da59e7d1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

        关于item的使用还请各位看一下文档，不是很难。
        https://doc.scrapy.org/en/1.3/topics/items.html
        这里贴一下我的items.py
        <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"> 8 from scrapy.item import Item,Field</div><div class="line">9</div><div class="line"> 11 class ZhilianJobItem(Item):</div><div class="line">   12     job_name = Field()</div><div class="line"> 13     job_company = Field()</div><div class="line">    14     job_price = Field()</div><div class="line">    15     job_position = Field()</div><div class="line">    16     job_date = Field()</div><div class="line">    17     job_is = Field()</div></pre></td></tr></table></figure>

                     ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-be42cf361b5b6027.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

                     运行一下:
                     <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl zhilian_spider</div></pre></td></tr></table></figure>

                     ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-53ef2e6ca8fb84fd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

                     这里我中断了，因为已经看到了想要的数据。
</code></pre><p>#####第七步 处理下一页问题<br>                               如果仔细分析，不难发现，我们只获取到了每个城市的第一页的数据，这很尴尬，如果有下一页怎么办，我们来处理一下。</p>
<pre><code>                           先看一下网页，找到下一页
                           ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-63c066290ab320bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

                           让我们回答parse_jobs方法
                           <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">                  #后去下一页的网址</div><div class="line"> 33             job_next = response.xpath(&apos;//a[@class=&quot;next-page&quot;]/@href&apos;).extract_first()</div><div class="line">                   #打印一下结果，因为之前并不知道job_next是什么值，所以打印一下看一下结果，发现如果没有就是None</div><div class="line">  34             print (&quot;********&quot;,job_next)</div><div class="line">                 #如果有下一页就爬取下一页</div><div class="line">35             if job_next is not None:</div><div class="line">                       #发送网络请求，并callback自己</div><div class="line"> 36                 yield scrapy.Request(url=job_next, callback=self.parse_jobs)</div></pre></td></tr></table></figure>

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-d3ab08afb569abc8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

&gt;关键在yield那句  callback我们回调的还是parse_jobs()方法

Ok，这样我们可以看一下结果。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scrapy crawl zhilian_spider</div></pre></td></tr></table></figure>

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-d6b3407b683856e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

没有问题。

那我们如何保存成文件呢。
只需要一句话 
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bogon:zhilian_job mac$ scrapy crawl zhilian_spider -o items.json</div></pre></td></tr></table></figure>

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-4541a708df8cc281.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

OK,到这第一部分就完成了。

&gt;补充，当你点开json文件，你会发现怎么全是unicode编码，说句实话 ，我也不知道怎么处理，使用item就是看到可以通过item解决，自己试了一下，并没有成功，但是这不影响后面的操作，如果有知道处理方式的可以共享一下。

下面我们会处理一下数据然后显示出来，敬请期待。

-----------------------有待更新-------------------------

OK，让我们继续下面的表演。

经过了大概半个小时，我终于看到了这个界面。恭喜你，你成功了，恭喜你，你被我坑了，哈哈，为什么这么说呢，下面给你解释。

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-55389ba32740c966.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

可以看到item_scraped_count是326153，也就是说你有一个32W+的一个数据文件了。

我们的目的是将其显示成图，哇，要显示这么多嘛？不是的，我们还需要处理，于是我们就需要使用pandas，做图必然要使用matplotlib。

这里我们只需要使用ipython就Ok了.

<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">	In [1]: import pandas as pd</div><div class="line">#注意这里我用用到的是pyplot 这里一定要引入matplotlib,后面会解释</div><div class="line">	In [2]: import matplotlib.pyplot as plt</div><div class="line"></div><div class="line">	In [3]: import numpy as np</div><div class="line"></div><div class="line">	In [4]: items = pd.read_json(&apos;items.json&apos;)</div><div class="line">#回车后你会发现有好多数据</div><div class="line">	In [5]: items</div></pre></td></tr></table></figure>

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-d86bbf540c359594.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

当时当我看到全国两个字的时候，我方了，为什么会有全国呢，我们不需要啊，而且你看到后面你会发现，我们估计花儿一半多时间才将全国爬取完成。也对不住各位坑你们了，让你们花了这么长时间，不过我感觉看着屏幕一直刷的感觉，像是在看骇客帝国，哈哈。其次，这样才有意思不是嘛？各种突发情况才更有趣。

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-e867b55740851d0b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

下面再说一下我们的需求
&gt;我们需要获取到每个城市的平均简历工资，我们需要获取到每个城市的招聘数量。
</code></pre><p>#####下面开始我们的数据处理的第一步，去重<br>    接下来我们要做的就是去重，依然使用pandas,我是在这篇文章中看到的。<a href="https://zhuanlan.zhihu.com/p/25473751？group_id=819874677425053696" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/25473751？group_id=819874677425053696</a></p>
<pre><code>![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-75228f0ed4febea5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Ok来执行以下
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">In [7]: items.drop_duplicates([&apos;job_name&apos;,&apos;job_position&apos;],keep=&apos;last&apos;,inplace=Tr</div><div class="line">		   ...: ue)</div><div class="line"></div><div class="line">In [8]: items</div></pre></td></tr></table></figure>

神奇的事情发生了，32W + 变成了4k+，看到这里，我貌似明白了什么。

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-8508c09c425e461f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们不难发现在处理工资这里的时候会有一些问题，有的是面议，其次就是xxxx - xxxx这样肯定算不出平均工资吧，我们还要处理一下。
</code></pre><p>#####第二步，数据再次处理</p>
<pre><code>当我纠结怎么办的时候，我突然发现文档里有一个叫做item pipeline的东西，貌似可以在里面处理再次处理数据。

我们在我们的Scrapy项目中找一下，发现在items.py同级目录下，有一个pipelines.py,既然加s说明可以有多个pipeline。

进去看一下，建议不要关闭ipython,重新打开一个终端。

<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">#导入re是因为发现工资是以字符串返回的，同事我们需要匹配格式为xxx-xxx的工资</div><div class="line">8 import re</div><div class="line">  9 from scrapy.exceptions import DropItem</div><div class="line">10 </div><div class="line"> 11 class ZhilianJobPipeline(object):</div><div class="line">   12     def process_item(self, item, spider):</div><div class="line">	    13         price = item[&quot;job_price&quot;]</div><div class="line">		 14         ma = re.match(r&apos;\d+-\d+&apos;,price)</div><div class="line">		    15         if ma is not None:</div><div class="line">				                #通过字符串的split方法将字符串分割获取得到最低与最高工资</div><div class="line">				 16             prices = price.split(&apos;-&apos;) </div><div class="line">				                 #这里需要转换一下，转换成int类型，然后算一下平均值</div><div class="line">				  17             price = (int(prices[0]) + int(prices[1])) / 2</div><div class="line">				   18             item[&apos;job_price&apos;] = price</div><div class="line">				                   #这样我们就获取到了每个招聘信息的平均价格。</div><div class="line">				    19             return item</div><div class="line">					 20         else:</div><div class="line">					  21             raise DropItem(&quot;Missing price in %s&quot; % item)</div></pre></td></tr></table></figure>

                              ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-78df69eefcaee243.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

                              这里还需要配置一个地方，我们需要设置setting.py里的ITEM_PIPELINE这个地方。文档最下面可以看到。因为我们这只有一个，所以我们只需要打开注释就好。
                              ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-3f2097256cb34912.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

                              ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-5ca58c68c492b769.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

                              这样就ok，保存一下，我们再来执行一下crawl zhilian_spider吧，不是吧，又要30分钟，是的，我当时就这样做的。想想也很无奈，不过第一次也是难免的。

                              开始前最好将之前的items.json删除。

                              <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Sheldon:zhilian_job mac$ rm -rf items.json </div><div class="line">Sheldon:zhilian_job mac$ scrapy crawl zhilian_spider -o items.json</div></pre></td></tr></table></figure>

                              ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-b2f2f04b49bfca7f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

                              没问题，这正是我们想要的。
</code></pre><p>#####第三步，数据整理<br>                                  回到第一步使用ipython对items进行去重，这里就不重复了。</p>
<pre><code>                              因为我们需要的是每个地区的所有招聘数量与平均工资，所以我们还需要对每个地区进行处理，还好在https://zhuanlan.zhihu.com/p/25473751?group_id=819874677425053696
                              这篇文章中我发现了一个叫做groupby的方法，貌似可以拿来用一下。

                              ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-fc87cf46121c1027.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

                              我们需要对position列进行分组，然后获取price的平均价格
                              这里是获取平均值的方法，http://www.aichengxu.com/python/16058.htm

                              ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-3b12da858ca99a96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

                              同时我们还需要显示个地区的所有招聘数量，这个可以使用count()方法。
                              ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-a7468b6d04877453.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

                              <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">In [13]: job_price = items.groupby(&apos;job_position&apos;)[&apos;job_price&apos;].mean()</div><div class="line"></div><div class="line">In [14]: job_num = items.groupby(&apos;job_position&apos;)[&apos;job_price&apos;].count()</div><div class="line"></div><div class="line">In [15]: job_num</div></pre></td></tr></table></figure>

                              ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-ebb3838ef5759399.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

                              数据我们算是初步处理完成，下面就是显示问题了，这里遇到一个坑，花了半天时间，真的是半天。

                              去看文档，上面写绘制图需要用到plot方法
                              http://pandas.pydata.org/pandas-docs/stable/visualization.html
                              然后我就试了一下

                              ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-2ed69f7c65c05d47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

                              &gt;然后呢？我没看到图啊 ,文档输入完plot()就显示图了，为什么我没有？

                              ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-d1271616e055ba2f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

                              &gt;然后找了半天资料，发现需要执行一下plt.show()这也是为什么一开始带入头文件时说一定要有plt。

                              <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">In [17]: plt.show()</div></pre></td></tr></table></figure>

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-a9301d729dd6f827.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

看到这张图的时候差点泪崩，文档有时候也骗人。不过仔细一看，就真的泪崩了，这是些啥？搞了这么就，就弄出来这个？其实细想其实是地区太多导致，显示不开。
于是我们可以改一下，只获取每个省的数量，这样的好处就是数量不多，同时，可以显示的开。于是回到spider中。

<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">11     def parse(self,response):</div><div class="line">  12 </div><div class="line">   13         citys = [&apos;北京&apos;,&apos;天津&apos;,&apos;上海&apos;,&apos;重庆&apos;,&apos;河北&apos;,&apos;山西&apos;,&apos;辽宁&apos;,&apos;吉林&apos;,&apos;黑</div><div class="line">      龙江&apos;,&apos;江苏&apos;,&apos;浙江&apos;,&apos;安徽&apos;,&apos;福建&apos;,&apos;江西&apos;,&apos;山东&apos;,&apos;河南&apos;,&apos;湖北&apos;,&apos;湖南&apos;,&apos;广东&apos;,    &apos;海南&apos;,&apos;四川&apos;,&apos;贵州&apos;,&apos;云南&apos;,&apos;陕西&apos;,&apos;甘肃&apos;,&apos;青海&apos;,&apos;台湾&apos;,&apos;内蒙古&apos;,&apos;广西&apos;,&apos;西&gt;    藏&apos;,&apos;宁夏&apos;,&apos;新疆&apos;,&apos;香港&apos;,&apos;澳门&apos;]</div><div class="line">   14         for ct in citys:</div><div class="line">   15         #for citys in response.xpath(&apos;//div[@id=&quot;letter_choose&quot;]/dl/dd&apos;):   </div><div class="line">   16             #for city in citys.xpath(&apos;.//a&apos;):</div><div class="line">   17                 #if city.xpath(&apos;.//strong&apos;).extract_first() is not None:</div><div class="line">   18                     #city = city.xpath(&apos;.//strong&apos;)</div><div class="line">   19                 #ct = city.xpath(&apos;.//text()&apos;).extract_first()</div><div class="line">   20             url_city = &apos;http://sou.zhaopin.com/jobs/searchresult.ashx?jl=%s&amp;    kw=ios&amp;sm=0&amp;p=1&apos; % ct</div><div class="line">   21             yield scrapy.Request(url=url_city, callback=self.parse_jobs)</div></pre></td></tr></table></figure>

      ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-f71ef9ba1471d3a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

      抓取结果:

      ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-62be3bab33013e66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

      不难发现，确实少了很多。
      然后去重，分租，就不重复了
      看一下最终结果:

      ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-25ca13b42aa01241.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

      这个是招聘平均工资，澳门香港最高。

      ![Paste_Image.png](http://upload-images.jianshu.io/upload_images/1503554-76dc4f4a9513dc96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

      看到整个山东加起来都不到150，我也就放心了。

      --------------------------  补充   ---------------------------------

      scrapy 文档：https://doc.scrapy.org/en/1.3/intro/tutorial.html#how-to-run-our-spider
      scrapy xpath文档：https://doc.scrapy.org/en/1.3/topics/selectors.html#topics-selectors  
      scrapy  items文档：http://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/items.html
      scrapy与mongoDB的使用：https://realpython.com/blog/python/web-scraping-with-scrapy-and-mongodb/
      scrapy 去重：https://zhuanlan.zhihu.com/p/25473751?group_id=819874677425053696
      pandas使用：http://pandas.pydata.org/pandas-docs/stable/10min.html#min
      pandas with matplotlib：http://pandas.pydata.org/pandas-docs/version/0.13.1/visualization.html
      pandas 读取json错误处理：http://stackoverflow.com/questions/36837663/reading-json-file-as-pandas-dataframe-error
</code></pre><p>####完！</p>

      
    </div>
    
    
      <footer class="article-footer">
        
      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2017/03/18/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Hello World&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>







</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 何小贱&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a href="http://github.com/ppoffice">PPOffice</a>
    </div>
  </div>
</footer>
    

<script src="/js/jquery.min.js"></script>



<script src="/js/script.js"></script>
  </div>
</body>
</html>